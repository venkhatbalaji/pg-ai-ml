{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40eb6f0c-7ef1-4d64-a8de-0dbf4384cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (if not already installed)\n",
    "!pip install torch --quiet\n",
    "!pip install transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c3cd3f1-e100-4420-b671-56d299b94a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Choose model tokenizer (GPT2, BERT, etc.)\n",
    "tokenizer_name = \"gpt2\"  # or \"EleutherAI/gpt-neo-125M\", etc.\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Add padding token if missing (for GPT2)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb163cfe-925d-48fe-b114-20a0a1f3f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class MultiHeadMaskedAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(x.device)\n",
    "        x, _ = self.attn(x, x, x, attn_mask=mask)\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadMaskedAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class GPTMiniHF(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, num_heads=4, ff_dim=1024, num_layers=6, max_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = PositionalEncoding(d_model, max_len)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            DecoderBlock(d_model, num_heads, ff_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b4d08cd-cb3b-47f2-aed6-7f36de1291c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=30):\n",
    "    model.eval()\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acbe669f-31f9-4595-b4fa-91e08754e4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m GPTMiniHF(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Test prompt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnce upon a time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "model = GPTMiniHF(vocab_size=vocab_size)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"Once upon a time\"\n",
    "output = generate_text(model, tokenizer, prompt, max_new_tokens=20)\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca3d7e0-5e18-4264-88c4-e6d35a5602c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
